{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4fZzXnRFW0i"
   },
   "source": [
    "# Kanapsack Problem databases 06:\n",
    "\n",
    "* https://people.sc.fsu.edu/~jburkardt/datasets/knapsack_01/knapsack_01.html\n",
    "\n",
    "# Atividade com nota:\n",
    "\n",
    "* Avaliar o algoritmo Hill Climbing para as bases P01 a P07;\n",
    "* Utilizar a função de aptidão knapsack do Mlrose;\n",
    "* Apresentar a melhor solução encontrada e comparar com a melhor solução global disponível para a base de dados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JNVoTIBnFONn",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlrose in /home/a10/anaconda3/lib/python3.11/site-packages (1.3.0)\r\n",
      "Requirement already satisfied: numpy in /home/a10/anaconda3/lib/python3.11/site-packages (from mlrose) (1.24.3)\r\n",
      "Requirement already satisfied: scipy in /home/a10/anaconda3/lib/python3.11/site-packages (from mlrose) (1.11.3)\r\n",
      "Requirement already satisfied: sklearn in /home/a10/anaconda3/lib/python3.11/site-packages (from mlrose) (0.0.post10)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install mlrose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RDMMYF51I3Tn"
   },
   "outputs": [],
   "source": [
    "import six\n",
    "import sys\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "import mlrose\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z12zZlrNI1L8"
   },
   "source": [
    "## Problem representation:\n",
    "### 0/1 knapsack problem using a hill climbing algorithm for P06.\n",
    "- The Problem: \n",
    "Given a set of items, each with a weight and a value, select a subset of the items to maximize the total value while keeping the total weight within a given capacity.\n",
    "\n",
    "So the total_weight of any system configuration (any state) is constraint with a max_weight, I will call this max_weight W.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BFtgAsHvwXUn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_weight = 170\n",
    "weights = [41,50,49,59,55,57,60]\n",
    "values = [442, 525, 511, 593, 546, 564,617]\n",
    "state = np.array([0, 1, 0, 1, 0, 0, 1])\n",
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zUWhR_neuJYs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Item0', 41, 442),\n",
       " ('Item1', 50, 525),\n",
       " ('Item2', 49, 511),\n",
       " ('Item3', 59, 593),\n",
       " ('Item4', 55, 546),\n",
       " ('Item5', 57, 564),\n",
       " ('Item6', 60, 617)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = []\n",
    "for i in range(len(weights)):\n",
    "    items.append(('Item' + str(i), weights[i], values[i]))\n",
    "\n",
    "items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This get_cost function is what I want to maximize. It's used to see which neighbor is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knapsack fitness function\n",
    "def get_cost(state):\n",
    "    total_value = sum(state[i] * values[i] for i in range(len(weights)))\n",
    "    total_weight = sum(state[i] * weights[i] for i in range(len(weights)))\n",
    "    if total_weight > max_weight:\n",
    "        return 0\n",
    "    return total_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is nothing but this in an equation form:\n",
    "\n",
    "$F(x_i) = \\sum_{i=0}^{n-1} x_i v_i$,    if $\\sum_{i=0}^{n-1} x_i w_i \\leq W$ this is the constraint\n",
    "\n",
    "\n",
    "Where $x_i$ represents a state vector $x = [x_0, x_1, \\ldots, x_{n-1}]$. It denotes a number of copies of item i included in the knapsack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitness value:  1735\n"
     ]
    }
   ],
   "source": [
    "# evaluate fitness function of given state\n",
    "total_value = get_cost(state)\n",
    "print(\"Fitness value: \", total_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom fitness function object\n",
    "fitness_cust = mlrose.CustomFitness(get_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create enviroment\n",
    "problem = mlrose.DiscreteOpt(length = 7, fitness_fn = fitness_cust, maximize = True, max_val = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hill Climb Method\n",
    "- Main Idea: \n",
    "The algorithm iteratively improves the solution by exploring neighboring states and selecting the state with the highest value (accordingly with get_cost) until no better solution can be found or a maximum number of iterations is reached (100 below).\n",
    "In other words, this algorithm maintain a single node and searches by moving to a neighboring node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 0, 1, 0, 0, 1]), 1735.0, array([], dtype=float64))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the hill_climb function to solve the problem\n",
    "best_state, best_fitness, curve = mlrose.hill_climb(problem, max_iters=100, restarts=0, init_state = state, curve=True)\n",
    "best_state, best_fitness, curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1735"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cost(best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soluçãoHC: [0 1 0 1 0 0 1]\n",
      "Fitness Value: 1735.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"soluçãoHC: {best_state}\")\n",
    "print(f\"Fitness Value: {best_fitness}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Random Restart \n",
    "Randomness should improve the optimization process value. Because for each iteration, it explores neighboring states by flipping the value (0 to 1 or 1 to 0) of a randomly selected item, and evaluates their costs and it keeps track of the best neighboring states with the highest cost.The current state is then updated to one of the best neighboring states. The process continues until no better neighbor is found or the maximum number of iterations is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 0, 1, 0, 0, 1]),\n",
       " 1735.0,\n",
       " array([   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0., 1139., 1139., 1581., 1581., 1581., 1581., 1581., 1581.,\n",
       "        1581., 1581., 1581., 1581., 1581.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0., 1582., 1582., 1582.,\n",
       "        1582., 1582., 1582., 1582., 1582., 1582., 1582., 1599., 1599.,\n",
       "        1599., 1599., 1599., 1599., 1599., 1599., 1599., 1599., 1599.,\n",
       "        1668., 1668., 1668., 1668., 1668., 1668., 1668., 1668., 1668.,\n",
       "        1668., 1668., 1629., 1629., 1629., 1629., 1629., 1629., 1629.,\n",
       "        1629., 1629., 1629., 1650., 1650., 1650., 1650., 1650., 1650.,\n",
       "        1650., 1650., 1650., 1650., 1621., 1621., 1621., 1621., 1621.,\n",
       "        1621., 1621., 1621., 1621., 1621., 1478., 1478., 1478., 1478.,\n",
       "        1478., 1478., 1478., 1478., 1478., 1478., 1735., 1735., 1735.,\n",
       "        1735., 1735., 1735., 1735., 1735., 1735., 1735., 1735.]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_state, best_fitness, curve = mlrose.random_hill_climb(problem, max_iters=100, restarts=10, curve=True, random_state=10)\n",
    "best_state, best_fitness, curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Rather than just hill climb one time, we can hill climb multiple times and figure out what is the best one that I've been able to find.\n",
    "So we've implemented a function for random restart that restarts some maximum number of times, and repeat, on that number of times, this hill-climb method 'random_hill_climb' and figure out what the cost is (for that state after randomly restart maximum times).\n",
    "As we can see there is a lot of local maximums (1139, 1581, 1582...),  the algorithm starts at 1139 and runs through a lot of neighbors, always keeping track (even the lowest ones) of the best neighboring states with the highest cost. 1735 is the global maximum for this dataset. So the given optimal solution was already a global maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated Annealing\n",
    "In order to find a global maximum or global minimum we may need to make a move that makes our situation worse. Because sometimes if we get stuck in a local max/min, I want to dislodge from that in order to find the global max/min.\n",
    " - Simulated Annealing is a great technique for that. \n",
    "Suppose some state-space, if I am in a current state and I'm looking for a global maximum and I'm trying to maximize the value of the state, Hill-Climbing would just take the state and look at the two neighbor ones, and always pick the one that is going to increase the value of the state. As said before \"in order to find a global maximum/minimum we may need to make a move that makes our situation worse.\" such that later on we can find that global maximum.\n",
    "Once found this global max state we probably don't wan't to be moving to states worse than current state. This is why this metaphor for annealing -> start making more random moves and, over time, start to make fewer of those random moves based on a particular temperature schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a schedule for simulated annealing (you can customize this schedule)\n",
    "schedule = mlrose.ExpDecay(init_temp=1000, exp_const=0.01, min_temp=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the idea is that this temperature is going to be higher early on and lower later on.\n",
    "It is exponentially decaying according to the formula: $T(t) = T_{i}e^{-rt}$\n",
    "where:\n",
    "-  T(t) is the temperature at time t.\n",
    "- $T(i)$ is the initial temperature at time t=0.\n",
    "- r is the rate of exponential decay.\n",
    "- t is the time variable.\n",
    "- The exponential decay is represented by $e^{-rt}$.\n",
    "\n",
    "For ex: You start off $T_{i}$=100, after 1s the temperature will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T(1) = 990.0498337491681\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the temperature parameter at time t = 1 and exp_const=0.01\n",
    "t1 = schedule.evaluate(1)\n",
    "print(f\"T(1) = {t1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 0, 1, 0, 0, 1]),\n",
       " 1735.0,\n",
       " array([   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0., 1650., 1057., 1621., 1621., 1621., 1621., 1075.,\n",
       "        1075., 1692., 1692., 1181., 1692., 1692., 1692., 1075., 1621.,\n",
       "        1621., 1621., 1621., 1621., 1621., 1621., 1621.,    0.,    0.,\n",
       "           0.,    0.,    0., 1706., 1706., 1181., 1692., 1128., 1653.,\n",
       "        1653., 1653., 1653., 1036., 1629., 1104., 1629., 1629., 1104.,\n",
       "        1629., 1629., 1629., 1629., 1629., 1629., 1629., 1629., 1118.,\n",
       "        1118., 1735., 1118., 1118., 1682., 1682., 1089., 1682., 1682.,\n",
       "        1682., 1682., 1682., 1682., 1682., 1682., 1157., 1682., 1118.,\n",
       "         593., 1210., 1210., 1735., 1735., 1210., 1210., 1210., 1721.,\n",
       "        1721., 1721., 1721., 1721., 1721., 1721., 1721., 1104., 1721.,\n",
       "        1104., 1546., 1546., 1546., 1546., 1546., 1546., 1546., 1546.,\n",
       "        1546., 1546., 1035., 1035., 1546., 1546., 1546., 1546., 1546.,\n",
       "         953., 1517., 1517., 1517., 1517., 1517., 1517.,  953., 1570.,\n",
       "        1570.,  953., 1570., 1570., 1570., 1570., 1570., 1570., 1059.,\n",
       "        1652., 1652., 1652., 1652., 1652., 1652., 1652., 1652., 1652.,\n",
       "        1652., 1652., 1652., 1652., 1652., 1210., 1210., 1735., 1735.,\n",
       "        1735., 1735., 1735., 1735., 1735., 1735., 1735., 1735., 1735.,\n",
       "        1735., 1735., 1735., 1735., 1735., 1735., 1735., 1735., 1735.,\n",
       "        1735., 1735., 1735., 1735., 1735., 1735., 1735., 1735., 1735.,\n",
       "        1735., 1735., 1735., 1735., 1735., 1735., 1735., 1735., 1735.,\n",
       "        1735., 1735., 1735., 1735., 1735., 1735., 1735., 1735., 1735.,\n",
       "        1735., 1735., 1735., 1735.]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the simulated_annealing function to solve the problem\n",
    "best_state, best_fitness, curve = mlrose.simulated_annealing(problem, schedule=schedule, max_attempts=50, init_state=None, curve=True, random_state=10)\n",
    "best_state, best_fitness, curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "so the goal of this whole process is as we begin our search to find the local/global max/min, we can dislodge ourselves if we get stuck at a local max/min in order to eventually make our way to exploring the part of the state space that is going to be the best. And then as the temperature decreases, start to make fewer of those random moves, that is, not moving aroung too much to get into another part of the state space.\n",
    "\n",
    "\n",
    "The simulated annealing algorithm ran through numerous iterations to find an optimal combination of items to include in a knapsack. The curve data indicates how the solution evolved over iterations, with the algorithm converging on the optimal solution towards the end.\n",
    "\n",
    "**Important**: In order for finding the global maximum, I had to **adjust the parameters** in the ExpDecay schedule and in the simulated_annealing function.\n",
    "\n",
    "- **Best State**: [0, 1, 0, 1, 0, 0, 1] This is an array representing the optimal solution found by the algorithm.\n",
    "- **Best fitness**: 1735 says that the combination of items in the best state yields a total value of 1735.\n",
    "- **Curve**: This is an array showing the fitness score at each iteration of the algorithm. It helps in understanding how the solution improved over time. The values fluctuate, indicating the exploration of different solutions. The repeated values of 1735 towards the end suggest that the algorithm consistently found a solution with a fitness of 1735, indicating it likely reached an optimal or near-optimal solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
