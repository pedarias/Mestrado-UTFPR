{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4fZzXnRFW0i"
   },
   "source": [
    "# Kanapsack Problem databases 01:\n",
    "\n",
    "* https://people.sc.fsu.edu/~jburkardt/datasets/knapsack_01/knapsack_01.html\n",
    "\n",
    "# Atividade com nota:\n",
    "\n",
    "* Avaliar o algoritmo Hill Climbing para as bases P01 a P07;\n",
    "* Utilizar a função de aptidão knapsack do Mlrose;\n",
    "* Apresentar a melhor solução encontrada e comparar com a melhor solução global disponível para a base de dados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "JNVoTIBnFONn",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlrose in /home/a10/anaconda3/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy in /home/a10/anaconda3/lib/python3.11/site-packages (from mlrose) (1.24.3)\n",
      "Requirement already satisfied: scipy in /home/a10/anaconda3/lib/python3.11/site-packages (from mlrose) (1.11.3)\n",
      "Requirement already satisfied: sklearn in /home/a10/anaconda3/lib/python3.11/site-packages (from mlrose) (0.0.post10)\n"
     ]
    }
   ],
   "source": [
    "!pip install mlrose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "RDMMYF51I3Tn"
   },
   "outputs": [],
   "source": [
    "import six\n",
    "import sys\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "import mlrose\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z12zZlrNI1L8"
   },
   "source": [
    "# 0/1 knapsack problem using a hill climbing algorithm for P01.\n",
    "- The Problem: \n",
    "Given a set of items, each with a weight and a value, select a subset of the items to maximize the total value while keeping the total weight within a given capacity.\n",
    "\n",
    "So the total_weight of any system configuration (any state) is constraint with a max_weight, I will call this max_weight W.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "BFtgAsHvwXUn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_weight = 165\n",
    "weights = [23, 31, 29, 44, 53, 38, 63, 85, 89, 82]\n",
    "values = [92,57,49,68,60,43,67,84,87,72]\n",
    "state = np.array([1, 1, 1, 1, 0, 1, 0, 0, 0, 0])\n",
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "zUWhR_neuJYs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Item0', 23, 92),\n",
       " ('Item1', 31, 57),\n",
       " ('Item2', 29, 49),\n",
       " ('Item3', 44, 68),\n",
       " ('Item4', 53, 60),\n",
       " ('Item5', 38, 43),\n",
       " ('Item6', 63, 67),\n",
       " ('Item7', 85, 84),\n",
       " ('Item8', 89, 87),\n",
       " ('Item9', 82, 72)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = []\n",
    "for i in range(len(weights)):\n",
    "    items.append(('Item' + str(i), weights[i], values[i]))\n",
    "\n",
    "items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This get_cost function is what I want to maximize. It's used to see which neighbor is better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fitness function\n",
    "def get_cost(state):\n",
    "    total_value = sum(state[i] * values[i] for i in range(len(weights)))\n",
    "    total_weight = sum(state[i] * weights[i] for i in range(len(weights)))\n",
    "    if total_weight > max_weight:\n",
    "        return 0\n",
    "    return total_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is nothing but this in an equation form:\n",
    "\n",
    "$F(x_i) = \\sum_{i=0}^{n-1} x_i v_i$,    if $\\sum_{i=0}^{n-1} x_i w_i \\leq W$ this is the constraint\n",
    "\n",
    "\n",
    "Where $x_i$ represents a state vector $x = [x_0, x_1, \\ldots, x_{n-1}]$. It denotes a number of copies of item i included in the knapsack.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profit: 309 \n"
     ]
    }
   ],
   "source": [
    "#sol otima\n",
    "#test_state = [0, 0, 1, 1, 1, 0, 1, 1, 0, 0]\n",
    "#state      = [1, 1, 1, 1, 0, 1, 0, 0, 0, 0]\n",
    "#total_value = get_cost(test_state)\n",
    "#print(f'{total_value}')\n",
    "total_value = get_cost(state)\n",
    "print(f'profit: {total_value} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom fitness function object\n",
    "fitness_cust = mlrose.CustomFitness(get_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating enviroment\n",
    "problem = mlrose.DiscreteOpt(length = 10, fitness_fn = fitness_cust, maximize = True, max_val = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hill Climb Method\n",
    "- Main Idea: \n",
    "The algorithm iteratively improves the solution by exploring neighboring states and selecting the state with the highest value (accordingly with get_cost) until no better solution can be found or a maximum number of iterations is reached (100 below).\n",
    "In other words, this algorithm maintain a single node and searches by moving to a neighboring node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 1, 1, 0, 1, 0, 0, 0, 0]), 309.0, array([], dtype=float64))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the hill_climb function to solve the problem\n",
    "best_state, best_fitness, curve = mlrose.hill_climb(problem, max_iters=100, restarts=0, init_state = state, curve=True)\n",
    "best_state, best_fitness, curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "309"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the fitness of the given state\n",
    "get_cost(best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SolucaoHC:  [1 1 1 1 0 1 0 0 0 0]\n",
      "Fitness Value: 309.0\n"
     ]
    }
   ],
   "source": [
    "print(\"SolucaoHC: \", best_state)\n",
    "print(\"Fitness Value:\", best_fitness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Random Restart \n",
    "Randomness should improve the optimization process value. Because for each iteration, it explores neighboring states by flipping the value (0 to 1 or 1 to 0) of a randomly selected item, and evaluates their costs and it keeps track of the best neighboring states with the highest cost.The current state is then updated to one of the best neighboring states. The process continues until no better neighbor is found or the maximum number of iterations is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 1, 1, 0, 1, 0, 0, 0, 0]),\n",
       " 309.0,\n",
       " array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        179., 179., 179., 179., 179., 179., 179., 179., 179., 179., 179.,\n",
       "        252., 252., 252., 252., 252., 252., 252., 309., 309., 309., 309.,\n",
       "        309., 309., 309., 309., 309., 309., 309.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0., 198., 266., 266., 266., 266., 266., 309.,\n",
       "        309., 309., 309., 309., 309., 309., 309., 309., 309., 309.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0., 217., 217., 217., 217., 217.,\n",
       "        217., 309., 309., 309., 309., 309., 309., 309., 309., 309., 309.,\n",
       "        309.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        178., 178., 178., 178., 270., 270., 270., 270., 270., 270., 270.,\n",
       "        270., 270., 270., 270.]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_state, best_fitness, curve = mlrose.random_hill_climb(problem, max_iters = 100, restarts = 10, curve=True, random_state=10)\n",
    "best_state, best_fitness, curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Rather than just hill climb one time, we can hill climb multiple times and figure out what is the best one that I've been able to find.\n",
    "So we've implemented a function for random restart that restarts some maximum number of times, and repeat, on that number of times, this hill-climb method 'random_hill_climb' and figure out what the cost is (for that state after randomly restart maximum times).\n",
    "As we can see there is a lot of local maximums (179, 252, 266...),  the algorithm starts at 0 and runs through a lot of neighbors, always keeping track (even the lowest ones) of the best neighboring states with the highest cost. 309 is the global maximum for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated Annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find a global maximum or global minimum we may need to make a move that makes our situation worse. Because sometimes if we get stuck in a local max/min, I want to dislodge from that in order to find the global max/min.\n",
    " - Simulated Annealing is a great technique for that. \n",
    "Suppose some state-space, if I am in a current state and I'm looking for a global maximum and I'm trying to maximize the value of the state, Hill-Climbing would just take the state and look at the two neighbor ones, and always pick the one that is going to increase the value of the state. As said before \"in order to find a global maximum/minimum we may need to make a move that makes our situation worse.\" such that later on we can find that global maximum.\n",
    "Once found this global max state we probably don't wan't to be moving to states worse than current state. This is why this metaphor for annealing -> start making more random moves and, over time, start to make fewer of those random moves based on a particular temperature schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a schedule for simulated annealing (you can customize this schedule)\n",
    "schedule = mlrose.ExpDecay(init_temp=1000, exp_const=0.01, min_temp=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the idea is that this temperature is going to be higher early on and lower later on.\n",
    "It is exponentially decaying according to the formula: $T(t) = T_{i}e^{-rt}$\n",
    "where:\n",
    "- $T(t)$ is the temperature at time t.\n",
    "- $T(i)$ is the initial temperature at time t=0.\n",
    "- r is the rate of exponential decay.\n",
    "- t is the time variable.\n",
    "- The exponential decay is represented by $e^{-rt}$.\n",
    "\n",
    "For ex: You start off $T_{i}$=100, after 1s the temperature will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T(1) = 990.0498337491681\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the temperature parameter at time t = 1 and exp_const=0.01\n",
    "t1 = schedule.evaluate(1)\n",
    "print(f\"T(1) = {t1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 1, 1, 0, 1, 0, 0, 0, 0]),\n",
       " 309.0,\n",
       " array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0., 181., 181.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0., 155., 212.,   0., 217.,   0.,   0.,   0., 236.,   0.,\n",
       "          0.,   0., 236.,   0.,   0.,   0., 209., 258., 166., 106., 149.,\n",
       "         92., 176., 176.,   0., 159., 251.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 147.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        244.,   0.,   0.,   0., 244., 244., 244., 244., 244., 244., 244.,\n",
       "        152., 152., 152., 220.,   0., 234., 234., 174.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0., 204., 136., 136.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0., 130., 130., 130.,  87., 147.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0., 178., 178., 106., 149., 216., 216.,\n",
       "        216., 149., 149., 241., 309., 309., 309., 241., 198., 198., 198.,\n",
       "        198., 141., 141., 201., 141., 225., 225., 176., 176., 176., 176.,\n",
       "        225., 225., 225., 225., 225., 225., 225., 225., 225., 225., 225.,\n",
       "        225., 225., 225., 225., 225., 225.,   0.,   0., 270., 270., 270.,\n",
       "        221., 221., 221., 221., 221., 221., 221., 221., 221., 221., 221.,\n",
       "        221., 149., 192., 241., 192., 241., 309., 309., 309., 309., 309.,\n",
       "        309., 309., 309., 309., 309., 309., 309., 309., 309., 309., 309.,\n",
       "        309., 309., 309., 309., 309.]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the simulated_annealing function to solve the problem\n",
    "best_state, best_fitness, curve = mlrose.simulated_annealing(problem, schedule=schedule, max_attempts=20, init_state=None, curve=True, random_state=1000)\n",
    "best_state, best_fitness, curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "So the goal of this whole process is as we begin our search to find the local/global max/min, we can dislodge ourselves if we get stuck at a local max/min in order to eventually make our way to exploring the part of the state space that is going to be the best. And then as the temperature decreases, start to make fewer of those random moves, that is, not moving aroung too much to get into another part of the state space.\n",
    "\n",
    "The simulated annealing algorithm ran through numerous iterations to find an optimal combination of items to include in a knapsack. The curve data indicates how the solution evolved over iterations, with the algorithm converging on the optimal solution towards the end.\n",
    "\n",
    "**Important**: In order for finding the global maximum, I had to **adjust the parameters** in the ExpDecay schedule and in the simulated_annealing function.\n",
    "\n",
    "- **Best State**: [1, 1, 1, 1, 0, 1, 0, 0, 0, 0] This is an array representing the optimal solution found by the algorithm.\n",
    "- **Best fitness**: 309 says that the combination of items in the best state yields a total value of 309.\n",
    "- **Curve**: This is an array showing the fitness score at each iteration of the algorithm. It helps in understanding how the solution improved over time. The values fluctuate, indicating the exploration of different solutions. The repeated values of 309 towards the end suggest that the algorithm consistently found a solution with a fitness of 309, indicating it likely reached an optimal or near-optimal solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Bonus**: The Hill Climb algorithm w/o mlrose\n",
    "### 0/1 knapsack problem using a hill climbing algorithm for P01.\n",
    "- The Problem: \n",
    "Given a set of items, each with a weight and a value, select a subset of the items to maximize the total value while keeping the total weight within a given capacity.\n",
    "\n",
    "- Main idea:\n",
    "The algorithm iteratively improves the solution by exploring neighboring states and selecting the state with the highest value until no better solution can be found or a maximum number of iterations is reached. The log parameter is used to log the progress during the execution of the algorithm.\n",
    "In other words, this algorithm maintain a single node and searches by moving to a neighboring node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a current state, based on this, take the best neighbor or set of all the best neighbors, and then choose from one of those\n",
    "import random\n",
    "\n",
    "\n",
    "class KnapsackSolver:\n",
    "    def __init__(self, weights, values, capacity):  # args we want to accept\n",
    "        self.weights = weights\n",
    "        self.values = values\n",
    "        self.capacity = capacity\n",
    "        self.n = len(weights)\n",
    "\n",
    "        # Initialize state with a given binary representation\n",
    "        self.state = [1, 1, 1, 1, 0, 1, 0, 0, 0, 0]  # Provided initial state\n",
    "    \n",
    "    #this cost is used to see which neighbor is better/ This method keeps track of the best neighboring states with the highest cost\n",
    "    #so I want to maximize it\n",
    "    def get_cost(self, state): # Calculate the total value of the selected item\n",
    "        total_value = sum(state[i] * self.values[i] for i in range(self.n))\n",
    "        # If the total weight exceeds the capacity, return a negative cost\n",
    "        total_weight = sum(state[i] * self.weights[i] for i in range(self.n))\n",
    "        if total_weight > self.capacity:\n",
    "            return -total_value\n",
    "        return total_value\n",
    "\n",
    "    def get_neighbors(self, state):\n",
    "        # Generate neighboring states by flipping the value (0 to 1 or 1 to 0) of a randomly selected item from some current state\n",
    "        neighbors = []\n",
    "        for i in range(self.n):\n",
    "            neighbor = state.copy()\n",
    "            neighbor[i] = 1 - neighbor[i]  # Flip the value\n",
    "            neighbors.append(neighbor)\n",
    "        return neighbors\n",
    "    \n",
    "    #HERE IS THE MAIN IDEA WHERE I WANT TO MAXIMIZE THE GET_COST FUNCTION IN ORDER TO FIND THE BEST NEIGHBOR\n",
    "    def hill_climb(self, maximum=None, log=False):\n",
    "        count = 0\n",
    "        \"\"\"function hill-climb(problem):\n",
    "              current = initial state of problem\n",
    "              repeat:\n",
    "                  neighbor = highest valued neighbor of current\n",
    "                  if neighbor not better than current:\n",
    "                      return current\n",
    "                  current = neighbor #if it's better\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        while maximum is None or count < maximum: #could specify maximum or run until hits a local max/min\n",
    "            count += 1 ##number of iterations\n",
    "            best_neighbors = []\n",
    "            best_neighbor_cost = None\n",
    "            #consider all neighbors for that state and the cost to it\n",
    "            for neighbor in self.get_neighbors(self.state):\n",
    "                cost = self.get_cost(neighbor)\n",
    "                #checks if neighbor is best so far\n",
    "                if best_neighbor_cost is None or cost > best_neighbor_cost:\n",
    "                    best_neighbor_cost = cost #update and\n",
    "                    best_neighbors = [neighbor] #keep track of best neighbor found\n",
    "                elif best_neighbor_cost == cost:\n",
    "                    best_neighbors.append(neighbor)\n",
    "\n",
    "            if best_neighbor_cost <= self.get_cost(self.state):\n",
    "                #best neighbor is worse than current state/ no better neighbor is found\n",
    "                return self.state\n",
    "\n",
    "            # Move to a highest-valued neighbor because above\n",
    "            self.state = random.choice(best_neighbors)\n",
    "\n",
    "            if log:\n",
    "                print(f\"Iteration {count}: Cost {best_neighbor_cost}\")\n",
    "\n",
    "        return self.state\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each iteration, it explores neighboring states by flipping the value (0 to 1 or 1 to 0) of a randomly selected item, and evaluates their costs and it keeps track of the best neighboring states with the highest cost.The current state is then updated to one of the best neighboring states.\n",
    "The process continues until no better neighbor is found or the maximum number of iterations is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with p01 parameters\n",
    "max_weight = 165\n",
    "weights = [23, 31, 29, 44, 53, 38, 63, 85, 89, 82]\n",
    "values = [92,57,49,68,60,43,67,84,87,72]\n",
    "initial_state = [1, 1, 1, 1, 0, 1, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = KnapsackSolver(weights, values, max_weight)\n",
    "best_solution = solver.hill_climb(log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Solution: [1, 1, 1, 1, 0, 1, 0, 0, 0, 0]\n",
      "Best Value: 309\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Solution:\", best_solution)\n",
    "print(\"Best Value:\", solver.get_cost(best_solution))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0/1 knapsack problem using a hill climbing algorithm with random restart for P05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rather than just hill climb one time, we can hill climb multiple times and figure out, what is the best one that I've been able to find.\n",
    "#So here I've implemented a function for random restart that restarts some maximum number of times\n",
    "#and repeat, on that number of times, this hill-climb method 'self.hill_climb()' and figure out what the cost is (for that state after randomly restart maximum times)\n",
    "import random\n",
    "\n",
    "\n",
    "class KnapsackSolver:\n",
    "    def __init__(self, weights, values, capacity):\n",
    "        self.weights = weights\n",
    "        self.values = values\n",
    "        self.capacity = capacity\n",
    "        self.n = len(weights)\n",
    "\n",
    "        # Initialize state with a random binary representation (0 or 1 for each item)\n",
    "        self.state = [random.randint(0, 1) for _ in range(self.n)]\n",
    "\n",
    "    def get_cost(self, state):\n",
    "        # Calculate the total value of the selected items\n",
    "        total_value = sum(state[i] * self.values[i] for i in range(self.n))\n",
    "        total_weight = sum(state[i] * self.weights[i] for i in range(self.n))\n",
    "        # If the total weight exceeds the capacity, return a negative cost\n",
    "        if total_weight > self.capacity:\n",
    "            return -total_value\n",
    "        return total_value\n",
    "\n",
    "    def get_neighbors(self, state):\n",
    "        # Generate neighboring states by flipping the value (0 to 1 or 1 to 0) of a randomly selected item\n",
    "        neighbors = []\n",
    "        for i in range(self.n):\n",
    "            neighbor = state.copy()\n",
    "            neighbor[i] = 1 - neighbor[i]  # Flip the value\n",
    "            neighbors.append(neighbor)\n",
    "        return neighbors\n",
    "\n",
    "    def hill_climb(self, maximum=None, log=False):\n",
    "        count = 0\n",
    "        best_state = self.state\n",
    "\n",
    "        while maximum is None or count < maximum:\n",
    "            count += 1\n",
    "            best_neighbors = []\n",
    "            best_neighbor_cost = None\n",
    "\n",
    "            for neighbor in self.get_neighbors(self.state):\n",
    "                cost = self.get_cost(neighbor)\n",
    "                if best_neighbor_cost is None or cost > best_neighbor_cost:\n",
    "                    best_neighbor_cost = cost\n",
    "                    best_neighbors = [neighbor]\n",
    "                elif best_neighbor_cost == cost:\n",
    "                    best_neighbors.append(neighbor)\n",
    "\n",
    "            if best_neighbor_cost <= self.get_cost(self.state):\n",
    "                # If no better neighbor is found return the same\n",
    "                return best_state\n",
    "\n",
    "            # Move to a highest-valued neighbor because above\n",
    "            self.state = random.choice(best_neighbors)\n",
    "            best_state = self.state\n",
    "\n",
    "            if log:\n",
    "                print(f\"Iteration {count}: Cost {best_neighbor_cost}\")\n",
    "\n",
    "        return best_state\n",
    "\n",
    "    def random_restart(self, maximum, log=False): #randomly restart maximum times\n",
    "        best_state = self.hill_climb()\n",
    "        best_cost = self.get_cost(best_state)\n",
    "\n",
    "        for i in range(maximum):\n",
    "            self.state = [random.randint(0, 1) for _ in range(self.n)]\n",
    "            state = self.hill_climb() #ATTEMPTING HILL CLIMBING FROM VARIOUS STARTING POINTS\n",
    "            cost = self.get_cost(state)\n",
    "\n",
    "            if cost > best_cost:\n",
    "                best_state = state\n",
    "                best_cost = cost\n",
    "\n",
    "            if log:\n",
    "                print(f\"Random Restart {i}: Cost {cost}\")\n",
    "\n",
    "        return best_state, best_cost\n",
    "\n",
    "#after running the hill-climbing algorithm on some particular, random initial configuration, we found a local max/min\n",
    "#This algorithm never make a move that makes our situation worse. It Always take current state, look at the neighbors and consider a better neighbor than our current state and move to one of those neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with provided parameters\n",
    "weights = [23, 31, 29, 44, 53, 38, 63, 85, 89, 82]\n",
    "values = [92,57,49,68,60,43,67,84,87,72]\n",
    "capacity = 165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Restart 0: Cost 216\n",
      "Random Restart 1: Cost 309\n",
      "Random Restart 2: Cost 284\n",
      "Random Restart 3: Cost 234\n",
      "Random Restart 4: Cost 170\n",
      "Random Restart 5: Cost 189\n",
      "Random Restart 6: Cost 204\n",
      "Random Restart 7: Cost 195\n",
      "Random Restart 8: Cost 284\n",
      "Random Restart 9: Cost 309\n",
      "Random Restart 10: Cost 154\n",
      "Random Restart 11: Cost 220\n",
      "Random Restart 12: Cost 224\n",
      "Random Restart 13: Cost 139\n",
      "Random Restart 14: Cost 239\n",
      "Random Restart 15: Cost 234\n",
      "Random Restart 16: Cost 209\n",
      "Random Restart 17: Cost 309\n",
      "Random Restart 18: Cost 195\n",
      "Random Restart 19: Cost 184\n",
      "Random Restart 20: Cost 216\n",
      "Random Restart 21: Cost 309\n",
      "Random Restart 22: Cost 232\n",
      "Random Restart 23: Cost 181\n",
      "Random Restart 24: Cost 224\n",
      "Random Restart 25: Cost 176\n",
      "Random Restart 26: Cost 195\n",
      "Random Restart 27: Cost 181\n",
      "Random Restart 28: Cost 184\n",
      "Random Restart 29: Cost 236\n",
      "Random Restart 30: Cost 276\n",
      "Random Restart 31: Cost 195\n",
      "Random Restart 32: Cost 276\n",
      "Random Restart 33: Cost 151\n",
      "Random Restart 34: Cost 259\n",
      "Random Restart 35: Cost 139\n",
      "Random Restart 36: Cost 209\n",
      "Random Restart 37: Cost 309\n",
      "Random Restart 38: Cost 216\n",
      "Random Restart 39: Cost 309\n",
      "Random Restart 40: Cost 284\n",
      "Random Restart 41: Cost 224\n",
      "Random Restart 42: Cost 309\n",
      "Random Restart 43: Cost 183\n",
      "Random Restart 44: Cost 216\n",
      "Random Restart 45: Cost 276\n",
      "Random Restart 46: Cost 284\n",
      "Random Restart 47: Cost 181\n",
      "Random Restart 48: Cost 197\n",
      "Random Restart 49: Cost 244\n"
     ]
    }
   ],
   "source": [
    "solver = KnapsackSolver(weights, values, capacity)\n",
    "best_solution, best_value = solver.random_restart(maximum=50, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Solution: [1, 1, 1, 1, 0, 1, 0, 0, 0, 0]\n",
      "Best Value: 309\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Solution:\", best_solution)\n",
    "print(\"Best Value:\", best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions:\n",
    "\n",
    "1. The first code is using mlrose library which we evaluate the fitness of a state vector.\n",
    "\n",
    " 2. The second part always starts with the same fixed initial state, which might lead to a local max/min depending on the problem. With its fixed initial state, may be more suitable for situations where you have a good initial estimate of the solution, and you want to fine-tune it using hill climbing without introducing randomness in the process.\n",
    "\n",
    " 3. In contrast, the third code, introduces randomness in the initial state, which can help explore different parts of the solution space. With its random_restart method, has the potential to find better solutions by attempting hill climbing from various starting points. This can be advantageous when dealing with a highly non-convex problem like the knapsack problem. As we found out after calculating a better state here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find a global maximum or global minimum we may need to make a move that makes our situation worse. Because sometimes if we get stuck in a local max/min, I want to dislodge from that in order to find the global max/min.\n",
    " - Simulated Annealing is a great technique for that. \n",
    "Suppose some state-space, if I am in a current state and I'm looking for a global maximum and I'm trying to maximize the value of the state, Hill-Climbing would just take the state and look at the two neighbor ones, and always pick the one that is going to increase the value of the state. As said before \"in order to find a global maximum/minimum we may need to make a move that makes our situation worse.\" such that later on we can find that global maximum.\n",
    "Once found this global max state we probably don't wan't to be moving to states worse than current state. This is why this metaphor for annealing -> start making more random moves and, over time, start to make fewer of those random moves based on a particular temperature schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
